{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc05c598",
   "metadata": {},
   "source": [
    "# MNIST Convolutional Neural Network (non-OOP approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1c01c",
   "metadata": {},
   "source": [
    "This notebook is actually an assignment for the CNN module in the Duke University's _Introduction to Machine Learning_ course on Coursera (great course by the way, check it out!). \n",
    "\n",
    "The assignment consisted in building a CNN that have the following properties: \n",
    "\n",
    "1. Image (28x28 pixels);\n",
    "2. Convolution, $C_{out} = 32$;\n",
    "3. (ReLU);\n",
    "4. Convolution, $C_{out} = 32$;\n",
    "5. (ReLU);\n",
    "6. 2x2 maxpool;\n",
    "7. Convolution, $C_{out} = 64$;\n",
    "8. (ReLU);\n",
    "9. Convolution, $C_{out} = 64$;\n",
    "10. (ReLU);\n",
    "11. 2x2 maxpool;\n",
    "12. fully connected hidden layer $(\\mathbb{R}^{256})$;\n",
    "13. (ReLU);\n",
    "14. fully connected hidden layer $(\\mathbb{R}^{10})$;\n",
    "15. softmax.\n",
    "\n",
    "As a challenge to myself (and because I did not know in what I was getting myself into) I decided to do it without the higher level API from nn.Module. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b38fc",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10784110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, \n",
    "                             transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, \n",
    "                            transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8b51c",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1709287",
   "metadata": {},
   "source": [
    "Trying to calculate the resulting dimensions after successive convolutions was giving me a headache, but it turns out thereâ€™s a closed-form expression for that.\n",
    "\n",
    "$$d = \\left\\lfloor\\frac{H + 2P-D(K-1) - 1}{S} + 1\\right\\rfloor$$\n",
    "\n",
    "Credits: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = 3\n",
    "K2 = 3\n",
    "K3 = 3\n",
    "K4 = 3\n",
    "\n",
    "MP_K1 = 2\n",
    "MP_K2 = 2\n",
    "\n",
    "P1 = 2\n",
    "P2 = 2\n",
    "P3 = 2\n",
    "P4 = 2\n",
    "\n",
    "S1 = 1\n",
    "S2 = 1\n",
    "S3 = 1\n",
    "S4 = 1\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "IN = 1\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_VEC = IMG_WIDTH*IMG_HEIGHT\n",
    "\n",
    "#output calcultions \n",
    "C1_OUT = 32\n",
    "SDIM_OUT1 =  int(np.floor((IMG_WIDTH + 2*P1-1*(K1-1)-1)/S1 + 1))\n",
    "C2_OUT = 32\n",
    "#after 2x2 maxpool \n",
    "SDIM_OUT2 = int(np.floor((SDIM_OUT1 + 2*P2-1*(K2-1)-1)/S2 + 1))//2\n",
    "C3_OUT = 64\n",
    "SDIM_OUT3 = int(np.floor((SDIM_OUT2 + 2*P3-1*(K3-1)-1)/S3 + 1))\n",
    "C4_OUT = 64\n",
    "#after 2x2 maxpool\n",
    "SDIM_OUT4 = int(np.floor((SDIM_OUT3 + 2*P4-1*(K4-1)-1)/S4 + 1))//2\n",
    "\n",
    "FC1_OUT = 256\n",
    "FC2_OUT = 10\n",
    "\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bdcb2",
   "metadata": {},
   "source": [
    "### Tensor definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "#convolutional layers\n",
    "\n",
    "#convolutional layer 1\n",
    "#batch_size = 100, 1 channel, 28x28 pixels\n",
    "w1 = torch.randn(C1_OUT, \n",
    "                 IN, \n",
    "                 K1, \n",
    "                 K1, \n",
    "                 requires_grad = True)\n",
    "w1.data = w1.data/np.sqrt(C1_OUT*K1**2)\n",
    "print(w1.shape)\n",
    "b1 = torch.zeros(C1_OUT, requires_grad = True)\n",
    "\n",
    "#convolutional layer 2\n",
    "w2 = torch.randn(C2_OUT, \n",
    "                 C1_OUT, \n",
    "                 K2, \n",
    "                 K2, \n",
    "                 requires_grad = True)\n",
    "\n",
    "w2.data = w2.data/np.sqrt(C2_OUT*K2**2)\n",
    "\n",
    "b2 = torch.zeros(C2_OUT, requires_grad = True)\n",
    "\n",
    "#convolutional layer 3\n",
    "w3 = torch.randn(C3_OUT, \n",
    "                 C2_OUT, \n",
    "                 K3, \n",
    "                 K3, \n",
    "                 requires_grad = True)\n",
    "\n",
    "w3.data = w3.data/np.sqrt(C3_OUT*K3**2)\n",
    "\n",
    "b3 = torch.zeros(C3_OUT, requires_grad = True)\n",
    "\n",
    "#convolutional layer 4\n",
    "w4 = torch.randn(C4_OUT, \n",
    "                 C3_OUT, \n",
    "                 K4, \n",
    "                 K4, \n",
    "                 requires_grad = True)\n",
    "\n",
    "w4.data = w4.data/np.sqrt(C4_OUT*K4**2)\n",
    "\n",
    "b4 = torch.zeros(C4_OUT, requires_grad = True)\n",
    "\n",
    "\n",
    "#fully connected layers\n",
    "#fc1\n",
    "w_fc1 = torch.randn(C4_OUT*SDIM_OUT4**2, \n",
    "                    FC1_OUT, \n",
    "                    requires_grad = True)\n",
    "w_fc1.data = w_fc1.data/np.sqrt(SDIM_OUT4)\n",
    "\n",
    "b_fc1 = torch.zeros(FC1_OUT, requires_grad = True)\n",
    "\n",
    "#fc2\n",
    "w_fc2 = torch.randn(FC1_OUT, \n",
    "                    FC2_OUT,\n",
    "                    requires_grad = True)\n",
    "w_fc2.data = w_fc2.data/np.sqrt(FC1_OUT)\n",
    "\n",
    "b_fc2 = torch.zeros(FC2_OUT, requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108a94f",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "699b1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward(x):\n",
    "    #first convolution\n",
    "    c1 = F.conv2d(x, w1, bias = b1, stride = S1, padding = P1)\n",
    "    c1 = F.relu(c1)\n",
    "    #print(\"c1: \", c1.shape)\n",
    "\n",
    "    #second convolution\n",
    "    c2 = F.conv2d(c1, w2, bias = b2, stride = S2, padding = P2)\n",
    "    c2 = F.relu(c2)    \n",
    "    c2 = F.max_pool2d(c2, kernel_size = MP_K1)\n",
    "    #print(\"c2: \", c2.shape)\n",
    "\n",
    "    #third convolution\n",
    "    c3 = F.conv2d(c2, w3, bias = b3, stride = S3, padding = P3)\n",
    "    c3 = F.relu(c3)\n",
    "    #print(\"c3: \", c3.shape)\n",
    "\n",
    "    #fourth convolution\n",
    "    c4 = F.conv2d(c3, w4, bias = b4, stride = S4, padding = P4)\n",
    "    c4 = F.relu(c4)\n",
    "    c4 = F.max_pool2d(c4, kernel_size = MP_K2)    \n",
    "    #print(\"c4: \", c4.shape)\n",
    "\n",
    "    x = c4.view(-1, C4_OUT*SDIM_OUT4**2)\n",
    "    x = torch.matmul(x, w_fc1) + b_fc1\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = torch.matmul(x, w_fc2) + b_fc2\n",
    "    #x = F.relu(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafa535",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([w1, b1, \n",
    "                              w2, b2,\n",
    "                              w3, b3,\n",
    "                              w4, b4,\n",
    "                              w_fc1, b_fc1, \n",
    "                              w_fc2, b_fc2], \n",
    "                              lr=0.001)\n",
    "for e in range(1, EPOCHS+1):\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        i += 1\n",
    "        if i%100 == 0: print(\"Epoch: {} | Batch number: {}\".format(e, i))\n",
    "        #reset the gradient at each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y = foward(images)\n",
    "        \n",
    "        #sigmoid built-in\n",
    "        cross_entropy = F.cross_entropy(y, labels)\n",
    "\n",
    "        cross_entropy.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a99c89",
   "metadata": {},
   "source": [
    "### Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9926999807357788\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "n_samples = len(mnist_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #go through the minibatchs \n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        #forward pass\n",
    "        y = foward(images)\n",
    "\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "\n",
    "        #right predictions \"boolean\" vector\n",
    "        predictions_vec = (predictions == labels)        \n",
    "        n_correct += torch.sum(predictions_vec)\n",
    "\n",
    "print('Test accuracy: {}'.format(n_correct/n_samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
